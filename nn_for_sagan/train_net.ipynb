{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.modeling import models, fitting\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../wuchengzhou')\n",
    "import sagan\n",
    "\n",
    "wave_dict = sagan.utils.line_wave_dict\n",
    "label_dict = sagan.utils.line_label_dict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform = np.random.uniform\n",
    "normal = np.random.normal\n",
    "\n",
    "def pnormal(mean, stddev):\n",
    "    while True:\n",
    "        value = normal(mean, stddev)\n",
    "        if value >= 0:  # 确保值不为负\n",
    "            return value\n",
    "\n",
    "arg_dict_func = {\n",
    "            'b_ha': {'amp_c':uniform, 'sigma_c':uniform, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "            'b_hb': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "            'b_hg': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal}, \n",
    "            'n_ha':{'amp_c':pnormal}, \n",
    "            'n_hb':{'amp_c':pnormal}, \n",
    "            'n_hc':{'amp_c':pnormal}, \n",
    "            'line_o3': {'amp_c0':pnormal, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "            'b_HeI': {'amp_c':pnormal, 'sigma_c':uniform, 'dv_c':normal}\n",
    "        }\n",
    "        \n",
    "arg_dict_range = {\n",
    "    'b_ha': {'amp_c':(1.5, 2.5), 'sigma_c':(1200, 1600), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.6), 'dv_w0':(0, 400), 'sigma_w0':(5000, 400)}, \n",
    "    'b_hb': {'amp_c':(0.7, 1.7), 'sigma_c':(1500, 200), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.3), 'dv_w0':(0, 100), 'sigma_w0':(5000, 450)}, \n",
    "    'b_hg': {'amp_c':(0.4, 0.9), 'sigma_c':(1500, 200), 'dv_c':(0, 75)}, \n",
    "    'n_ha':{'amp_c':(0.1, 0.05)}, \n",
    "    'n_hb':{'amp_c':(0.1, 0.05)}, \n",
    "    'n_hc':{'amp_c':(0.1, 0.05)}, \n",
    "    'line_o3': {'amp_c0':(1, 0.5), 'sigma_c':(500, 200), 'dv_c':(0, 75), 'amp_w0':(0.1, 0.5), 'dv_w0':(-100, 100), 'sigma_w0':(1700, 400)}, \n",
    "    'b_HeI': {'amp_c':(0.1, 0.08), 'sigma_c':(1400, 1800), 'dv_c':(0, 75)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def spec_gen functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_continuum(wave):\n",
    "    # Generate random parameters for the power law\n",
    "    amp1 = 10 * np.random.rand()\n",
    "    amp2 = np.random.rand()\n",
    "    alpha = uniform(0, 2)\n",
    "    stddev = uniform(500, 2500)\n",
    "    z = uniform(0, 0.01)\n",
    "    \n",
    "    # Create the model\n",
    "    pl_amps = models.PowerLaw1D(amplitude=amp1, x_0=5500, alpha=alpha, fixed={'x_0': True})\n",
    "    iron = sagan.IronTemplate(amplitude=amp2, stddev=stddev, z=z, name='Fe II')\n",
    "    model = pl_amps + iron\n",
    "    flux = model(wave)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 0.1, wave.size)\n",
    "    flux += noise\n",
    "    \n",
    "    return flux\n",
    "\n",
    "# narrow Line with 2 components\n",
    "# Hb:2, oIII:2, narrow: 1, Ha: 2\n",
    "def generate_spec(wave, arg_dict):\n",
    "\n",
    "    amp_c0 = arg_dict['line_o3']['amp_c0']\n",
    "    dv_c = arg_dict['line_o3']['dv_c']\n",
    "    sigma_c = arg_dict['line_o3']['sigma_c']\n",
    "    amp_w0 = arg_dict['line_o3']['amp_w0']\n",
    "    dv_w0 = arg_dict['line_o3']['dv_w0']\n",
    "    sigma_w0 = arg_dict['line_o3']['sigma_w0']\n",
    "\n",
    "    line_o3 = sagan.Line_MultiGauss_doublet(n_components=2, amp_c0=amp_c0, amp_c1=0.2, dv_c=dv_c, sigma_c=sigma_c, wavec0=wave_dict['OIII_5007'], wavec1=wave_dict['OIII_4959'], name='[O III]', amp_w0=amp_w0, dv_w0=dv_w0, sigma_w0=sigma_w0)\n",
    "    \n",
    "    def tie_o3(model):\n",
    "        return model['[O III]'].amp_c0 / 2.98\n",
    "    line_o3.amp_c1.tied = tie_o3\n",
    "    \n",
    "    n_ha = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['n_ha']['amp_c'], wavec=wave_dict['Halpha'], name=f'narrow {label_dict[\"Halpha\"]}')\n",
    "    n_hb = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['n_hb']['amp_c'], wavec=wave_dict['Hbeta'], name=f'narrow {label_dict[\"Hbeta\"]}')\n",
    "    n_hg = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['n_hc']['amp_c'], wavec=wave_dict['Hgamma'], name=f'narrow {label_dict[\"Hgamma\"]}')\n",
    "\n",
    "    \n",
    "    b_HeI = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['b_HeI']['amp_c'], dv_c=arg_dict['b_HeI']['dv_c'], sigma_c=arg_dict['b_HeI']['sigma_c'], wavec=5875.624, name=f'He I 5876')\n",
    "    \n",
    "    b_ha = sagan.Line_MultiGauss(n_components=2, amp_c=arg_dict['b_ha']['amp_c'], dv_c=arg_dict['b_ha']['dv_c'], sigma_c=arg_dict['b_ha']['sigma_c'], wavec=wave_dict['Halpha'], name=label_dict['Halpha'], amp_w0=arg_dict['b_ha']['amp_w0'], sigma_w0=arg_dict['b_ha']['sigma_w0'], dv_w0=arg_dict['b_ha']['dv_w0'])\n",
    "    b_hb = sagan.Line_MultiGauss(n_components=2, amp_c=arg_dict['b_hb']['amp_c'], dv_c=arg_dict['b_hb']['dv_c'], sigma_c=arg_dict['b_hb']['sigma_c'], wavec=wave_dict['Hbeta'], name=label_dict['Hbeta'], amp_w0=arg_dict['b_hb']['amp_w0'], dv_w0=arg_dict['b_hb']['dv_w0'], sigma_w0=arg_dict['b_hb']['sigma_w0'])\n",
    "    b_hg = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['b_hg']['amp_c'], dv_c=arg_dict['b_hg']['dv_c'], sigma_c=arg_dict['b_hg']['sigma_c'], wavec=wave_dict['Hgamma'], name=label_dict['Hgamma'])\n",
    "    \n",
    "    def tie_narrow_sigma_c(model):\n",
    "        return model['[O III]'].sigma_c\n",
    "\n",
    "    def tie_narrow_dv_c(model):\n",
    "        return model['[O III]'].dv_c\n",
    "\n",
    "    for line in [n_ha, n_hb, n_hg]:\n",
    "        line.sigma_c.tied = tie_narrow_sigma_c\n",
    "        line.dv_c.tied = tie_narrow_dv_c\n",
    "    \n",
    "    line_ha = b_ha + n_ha\n",
    "    line_hb = b_hb + n_hb\n",
    "    line_hg = b_hg + n_hg\n",
    "\n",
    "    # def model\n",
    "    model = (line_ha + line_hb + line_hg + line_o3 + b_HeI)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = np.random.normal(0, 0.015, wave.size)\n",
    "    \n",
    "    flux = model(wave) + noise\n",
    "    \n",
    "    return flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def neural network and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self, input_height=2, input_width=1000, output_dim=27):\n",
    "        super(CNN1, self).__init__()\n",
    "        # 输入为2×n的矩阵，假设n=100（可以根据实际情况调整）\n",
    "        self.bn0 = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 21), padding=(0, 10))  # 输入通道1，输出通道16\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 21), padding=(0, 10))  # 输入通道16，输出通道32\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(1, 21), padding=(0, 10))  # 输入通道32，输出通道64\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))         # 池化层\n",
    "        # 计算全连接层的输入维度\n",
    "        self.fc_input_dim = self._calculate_fc_input_dim(input_height, input_width)\n",
    "        self.fc1 = nn.Linear(self.fc_input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 假设输入x的维度为[batch_size, 1, 2, n]\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # 展平操作\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def _calculate_fc_input_dim(self, input_height, input_width):\n",
    "        # 计算卷积层和池化层后的特征图尺寸\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_height, input_width)\n",
    "            dummy_output = self.pool(F.relu(self.conv3(self.pool(F.relu(self.conv2(self.pool(F.relu(self.conv1(dummy_input)))))))))\n",
    "            return dummy_output.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss1(nn.Module):\n",
    "    def __init__(self, arg_dict_func, arg_dict_range):\n",
    "        super(Loss1, self).__init__()\n",
    "        self.arg_dict_func = arg_dict_func\n",
    "        self.arg_dict_range = arg_dict_range\n",
    "        self.w = []\n",
    "        for key1, line in arg_dict_func.items():\n",
    "            for key2, value in line.items():\n",
    "                if value == uniform:\n",
    "                    self.w.append(arg_dict_range[key1][key2][1] - arg_dict_range[key1][key2][0])\n",
    "                elif value == pnormal or value == normal:\n",
    "                    self.w.append(arg_dict_range[key1][key2][1])\n",
    "        self.w = torch.tensor(self.w, dtype=torch.float32).to(device)\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return x / self.w\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        targets_norm = self.normalize(targets)\n",
    "        loss = torch.mean((outputs - targets_norm) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader):\n",
    "    loss = Loss1(arg_dict_func, arg_dict_range)\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            # 计算相对误差\n",
    "            relative_error = torch.abs(outputs * loss.w - targets) / torch.abs(targets)\n",
    "            correct = torch.all(relative_error < 0.5, dim=1).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += inputs.size(0)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "            \n",
    "        # 计算训练集和测试集上的准确率\n",
    "        train_accuracy = calculate_accuracy(model, train_loader)\n",
    "        test_accuracy = calculate_accuracy(model, test_loader)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "def plot_training_curve(train_losses, train_accuracies, test_accuracies):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(arg_dict_func, arg_dict_range, num_samples=200, input_width=1000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        \n",
    "        # arg_dict_func = {\n",
    "        #     'b_ha': {'amp_c':uniform, 'sigma_c':uniform, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "        #     'b_hb': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "        #     'b_hg': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal}, \n",
    "        #     'n_ha':{'amp_c':pnormal}, \n",
    "        #     'n_hb':{'amp_c':pnormal}, \n",
    "        #     'n_hc':{'amp_c':pnormal}, \n",
    "        #     'line_o3': {'amp_c0':pnormal, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "        #     'b_HeI': {'amp_c':pnormal, 'sigma_c':uniform, 'dv_c':normal}\n",
    "        # }\n",
    "        \n",
    "        # arg_dict_range = {\n",
    "        #     'b_ha': {'amp_c':(1.5, 2.5), 'sigma_c':(1200, 1600), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.6), 'dv_w0':(0, 400), 'sigma_w0':(5000, 400)}, \n",
    "        #     'b_hb': {'amp_c':(0.7, 1.7), 'sigma_c':(1500, 200), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.3), 'dv_w0':(0, 100), 'sigma_w0':(5000, 450)}, \n",
    "        #     'b_hg': {'amp_c':(0.4, 0.9), 'sigma_c':(1500, 200), 'dv_c':(0, 75)}, \n",
    "        #     'n_ha':{'amp_c':(0.1, 0.05)}, \n",
    "        #     'n_hb':{'amp_c':(0.1, 0.05)}, \n",
    "        #     'n_hc':{'amp_c':(0.1, 0.05)}, \n",
    "        #     'line_o3': {'amp_c0':(1, 0.5), 'sigma_c':(500, 200), 'dv_c':(0, 75), 'amp_w0':(0.1, 0.5), 'dv_w0':(-100, 100), 'sigma_w0':(1700, 400)}, \n",
    "        #     'b_HeI': {'amp_c':(0.1, 0.08), 'sigma_c':(1400, 1800), 'dv_c':(0, 75)}\n",
    "        # }\n",
    "        \n",
    "        arg_dict = {key: {param: arg_dict_func[key][param](*arg_dict_range[key][param]) for param in arg_dict_func[key]} for key in arg_dict_func}\n",
    "        \n",
    "        wave = np.linspace(4150, 7000, input_width)\n",
    "        flux = generate_spec(wave, arg_dict=arg_dict)\n",
    "        \n",
    "        data = np.stack((wave, flux), axis=0)\n",
    "        X_list.append(torch.tensor(data, dtype=torch.float32).view(1, 2, input_width))\n",
    "        arg_list = [value for line in arg_dict.values() for value in line.values()]\n",
    "        y_list.append(torch.tensor(arg_list, dtype=torch.float32))\n",
    "    \n",
    "    X = torch.cat(X_list, dim=0).reshape(num_samples, 1, 2, input_width)\n",
    "    y = torch.stack(y_list)\n",
    "\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功加载。\n",
      "model loaded\n",
      "Epoch [1/400], Loss: 0.3016, Train Accuracy: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch [11/400], Loss: 0.6990, Train Accuracy: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch [21/400], Loss: 0.6990, Train Accuracy: 0.0000, Test Accuracy: 0.0000\n",
      "Epoch [31/400], Loss: 0.6990, Train Accuracy: 0.0000, Test Accuracy: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m train_losses, train_accuracies, test_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 绘制训练过程\u001b[39;00m\n\u001b[0;32m     45\u001b[0m plot_training_curve(train_losses, train_accuracies, test_accuracies)\n",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     31\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 32\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\sagan\\Lib\\site-packages\\torch\\optim\\optimizer.py:478\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    477\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# call optimizer step pre hooks\u001b[39;00m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pre_hook \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[0;32m    481\u001b[0m         _global_optimizer_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_pre_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    483\u001b[0m     ):\n\u001b[0;32m    484\u001b[0m         result \u001b[38;5;241m=\u001b[39m pre_hook(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\sagan\\Lib\\site-packages\\torch\\autograd\\profiler.py:769\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 769\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\sagan\\Lib\\site-packages\\torch\\_ops.py:960\u001b[0m, in \u001b[0;36mTorchBindOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_as_effectful_op_temporarily():\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_in_python(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fallthrough_keys())\n\u001b[1;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 超参数\n",
    "    input_height = 2\n",
    "    input_width = 1000\n",
    "    output_dim = 27\n",
    "    num_samples = 114514\n",
    "    batch_size = 32\n",
    "    num_epochs = 400\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    try:\n",
    "        X = torch.load(\"./data_generated/X.pt\")\n",
    "        y = torch.load(\"./data_generated/y.pt\")\n",
    "        print(\"数据已成功加载。\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"未找到数据，正在生成数据...\")\n",
    "        X, y = generate_data(arg_dict_func, arg_dict_range, num_samples, input_width)\n",
    "        # 保存数据\n",
    "        torch.save(X, \"./data_generated/X.pt\")\n",
    "        torch.save(y, \"./data_generated/y.pt\")\n",
    "        print(\"数据已成功生成并保存。\")\n",
    "    \n",
    "    dataset = TensorDataset(X.to(device), y.to(device))\n",
    "    train_size = int(0.8 * num_samples)\n",
    "    test_size = num_samples - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = CNN1(input_height, input_width, output_dim).to(device)\n",
    "    model.load_state_dict(torch.load(\"./model/cnn1.pth\"))\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    # criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "    criterion = Loss1(arg_dict_func, arg_dict_range)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 训练模型\n",
    "    train_losses, train_accuracies, test_accuracies = train_model(model, train_loader, test_loader, criterion,\n",
    "                                                                  optimizer, num_epochs)\n",
    "\n",
    "    # 绘制训练过程\n",
    "    plot_training_curve(train_losses, train_accuracies, test_accuracies)\n",
    "\n",
    "    # 保存模型\n",
    "    model_name = 'cnn1_1'\n",
    "    torch.save(model.state_dict(), f\"./model/{model_name}.pth\")\n",
    "    print(f\"Model saved to ./model/{model_name}.pth\")\n",
    "    \n",
    "    # 加载模型并测试\n",
    "    model.load_state_dict(torch.load(f\"./model/{model_name}.pth\"))\n",
    "    test_accuracy = calculate_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/cnn1.pth\"\n",
    "model = CNN1(input_height, input_width, output_dim).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "X_test, y_target = generate_data(arg_dict_func, arg_dict_range, 1, input_width)\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "y_target = y_target.to(device)\n",
    "\n",
    "y_outputs = model(X_test)\n",
    "print(y_target)\n",
    "print(y_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
