{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.modeling import models, fitting\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../wuchengzhou')\n",
    "import sagan\n",
    "\n",
    "wave_dict = sagan.utils.line_wave_dict\n",
    "label_dict = sagan.utils.line_label_dict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform = np.random.uniform\n",
    "normal = np.random.normal\n",
    "\n",
    "def pnormal(mean, stddev):\n",
    "    while True:\n",
    "        value = normal(mean, stddev)\n",
    "        if value >= 0:  # 确保值不为负\n",
    "            return value\n",
    "\n",
    "arg_dict_func = {\n",
    "            'b_ha': {'amp_c':uniform, 'sigma_c':uniform, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "            'b_hb': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "            'b_hg': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal}, \n",
    "            'n_ha':{'amp_c':pnormal}, \n",
    "            'n_hb':{'amp_c':pnormal}, \n",
    "            'n_hc':{'amp_c':pnormal}, \n",
    "            'line_o3': {'amp_c0':pnormal, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "            'b_HeI': {'amp_c':pnormal, 'sigma_c':uniform, 'dv_c':normal}\n",
    "        }\n",
    "        \n",
    "arg_dict_range = {\n",
    "    'b_ha': {'amp_c':(1.5, 2.5), 'sigma_c':(1200, 1600), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.6), 'dv_w0':(0, 400), 'sigma_w0':(5000, 400)}, \n",
    "    'b_hb': {'amp_c':(0.7, 1.7), 'sigma_c':(1500, 200), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.3), 'dv_w0':(0, 100), 'sigma_w0':(5000, 450)}, \n",
    "    'b_hg': {'amp_c':(0.4, 0.9), 'sigma_c':(1500, 200), 'dv_c':(0, 75)}, \n",
    "    'n_ha':{'amp_c':(0.1, 0.05)}, \n",
    "    'n_hb':{'amp_c':(0.1, 0.05)}, \n",
    "    'n_hc':{'amp_c':(0.1, 0.05)}, \n",
    "    'line_o3': {'amp_c0':(1, 0.5), 'sigma_c':(500, 200), 'dv_c':(0, 75), 'amp_w0':(0.1, 0.5), 'dv_w0':(-100, 100), 'sigma_w0':(1700, 400)}, \n",
    "    'b_HeI': {'amp_c':(0.1, 0.08), 'sigma_c':(1400, 1800), 'dv_c':(0, 75)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuum(wave):\n",
    "    # Generate random parameters for the power law\n",
    "    amp1 = 10 * np.random.rand()\n",
    "    amp2 = np.random.rand()\n",
    "    alpha = uniform(0, 2)\n",
    "    stddev = uniform(500, 2500)\n",
    "    z = uniform(0, 0.01)\n",
    "    \n",
    "    # Create the model\n",
    "    pl_amps = models.PowerLaw1D(amplitude=amp1, x_0=5500, alpha=alpha, fixed={'x_0': True})\n",
    "    iron = sagan.IronTemplate(amplitude=amp2, stddev=stddev, z=z, name='Fe II')\n",
    "    model = pl_amps + iron\n",
    "    flux = model(wave)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 0.1, wave.size)\n",
    "    flux += noise\n",
    "    \n",
    "    return flux\n",
    "\n",
    "# narrow Line with 2 components\n",
    "# Hb:2, oIII:2, narrow: 1, Ha: 2\n",
    "def generate_spec(wave, arg_dict):\n",
    "\n",
    "    amp_c0 = arg_dict['line_o3']['amp_c0']\n",
    "    dv_c = arg_dict['line_o3']['dv_c']\n",
    "    sigma_c = arg_dict['line_o3']['sigma_c']\n",
    "    amp_w0 = arg_dict['line_o3']['amp_w0']\n",
    "    dv_w0 = arg_dict['line_o3']['dv_w0']\n",
    "    sigma_w0 = arg_dict['line_o3']['sigma_w0']\n",
    "\n",
    "    line_o3 = sagan.Line_MultiGauss_doublet(n_components=2, amp_c0=amp_c0, amp_c1=0.2, dv_c=dv_c, sigma_c=sigma_c, wavec0=wave_dict['OIII_5007'], wavec1=wave_dict['OIII_4959'], name='[O III]', amp_w0=amp_w0, dv_w0=dv_w0, sigma_w0=sigma_w0)\n",
    "    \n",
    "    def tie_o3(model):\n",
    "        return model['[O III]'].amp_c0 / 2.98\n",
    "    line_o3.amp_c1.tied = tie_o3\n",
    "    \n",
    "    n_ha = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['n_ha']['amp_c'], wavec=wave_dict['Halpha'], name=f'narrow {label_dict[\"Halpha\"]}')\n",
    "    n_hb = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['n_hb']['amp_c'], wavec=wave_dict['Hbeta'], name=f'narrow {label_dict[\"Hbeta\"]}')\n",
    "    n_hg = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['n_hc']['amp_c'], wavec=wave_dict['Hgamma'], name=f'narrow {label_dict[\"Hgamma\"]}')\n",
    "\n",
    "    \n",
    "    b_HeI = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['b_HeI']['amp_c'], dv_c=arg_dict['b_HeI']['dv_c'], sigma_c=arg_dict['b_HeI']['sigma_c'], wavec=5875.624, name=f'He I 5876')\n",
    "    \n",
    "    b_ha = sagan.Line_MultiGauss(n_components=2, amp_c=arg_dict['b_ha']['amp_c'], dv_c=arg_dict['b_ha']['dv_c'], sigma_c=arg_dict['b_ha']['sigma_c'], wavec=wave_dict['Halpha'], name=label_dict['Halpha'], amp_w0=arg_dict['b_ha']['amp_w0'], sigma_w0=arg_dict['b_ha']['sigma_w0'], dv_w0=arg_dict['b_ha']['dv_w0'])\n",
    "    b_hb = sagan.Line_MultiGauss(n_components=2, amp_c=arg_dict['b_hb']['amp_c'], dv_c=arg_dict['b_hb']['dv_c'], sigma_c=arg_dict['b_hb']['sigma_c'], wavec=wave_dict['Hbeta'], name=label_dict['Hbeta'], amp_w0=arg_dict['b_hb']['amp_w0'], dv_w0=arg_dict['b_hb']['dv_w0'], sigma_w0=arg_dict['b_hb']['sigma_w0'])\n",
    "    b_hg = sagan.Line_MultiGauss(n_components=1, amp_c=arg_dict['b_hg']['amp_c'], dv_c=arg_dict['b_hg']['dv_c'], sigma_c=arg_dict['b_hg']['sigma_c'], wavec=wave_dict['Hgamma'], name=label_dict['Hgamma'])\n",
    "    \n",
    "    def tie_narrow_sigma_c(model):\n",
    "        return model['[O III]'].sigma_c\n",
    "\n",
    "    def tie_narrow_dv_c(model):\n",
    "        return model['[O III]'].dv_c\n",
    "\n",
    "    for line in [n_ha, n_hb, n_hg]:\n",
    "        line.sigma_c.tied = tie_narrow_sigma_c\n",
    "        line.dv_c.tied = tie_narrow_dv_c\n",
    "    \n",
    "    line_ha = b_ha + n_ha\n",
    "    line_hb = b_hb + n_hb\n",
    "    line_hg = b_hg + n_hg\n",
    "\n",
    "    # def model\n",
    "    model = (line_ha + line_hb + line_hg + line_o3 + b_HeI)\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = np.random.normal(0, 0.015, wave.size)\n",
    "    \n",
    "    flux = model(wave) + noise\n",
    "    \n",
    "    return flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(arg_dict_func, arg_dict_range, num_samples=200, input_width=1000):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        \n",
    "        # arg_dict_func = {\n",
    "        #     'b_ha': {'amp_c':uniform, 'sigma_c':uniform, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "        #     'b_hb': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "        #     'b_hg': {'amp_c':uniform, 'sigma_c':pnormal, 'dv_c':normal}, \n",
    "        #     'n_ha':{'amp_c':pnormal}, \n",
    "        #     'n_hb':{'amp_c':pnormal}, \n",
    "        #     'n_hc':{'amp_c':pnormal}, \n",
    "        #     'line_o3': {'amp_c0':pnormal, 'sigma_c':pnormal, 'dv_c':normal, 'amp_w0':uniform, 'dv_w0':normal, 'sigma_w0':pnormal}, \n",
    "        #     'b_HeI': {'amp_c':pnormal, 'sigma_c':uniform, 'dv_c':normal}\n",
    "        # }\n",
    "        \n",
    "        # arg_dict_range = {\n",
    "        #     'b_ha': {'amp_c':(1.5, 2.5), 'sigma_c':(1200, 1600), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.6), 'dv_w0':(0, 400), 'sigma_w0':(5000, 400)}, \n",
    "        #     'b_hb': {'amp_c':(0.7, 1.7), 'sigma_c':(1500, 200), 'dv_c':(0, 75), 'amp_w0':(0.05, 0.3), 'dv_w0':(0, 100), 'sigma_w0':(5000, 450)}, \n",
    "        #     'b_hg': {'amp_c':(0.4, 0.9), 'sigma_c':(1500, 200), 'dv_c':(0, 75)}, \n",
    "        #     'n_ha':{'amp_c':(0.1, 0.05)}, \n",
    "        #     'n_hb':{'amp_c':(0.1, 0.05)}, \n",
    "        #     'n_hc':{'amp_c':(0.1, 0.05)}, \n",
    "        #     'line_o3': {'amp_c0':(1, 0.5), 'sigma_c':(500, 200), 'dv_c':(0, 75), 'amp_w0':(0.1, 0.5), 'dv_w0':(-100, 100), 'sigma_w0':(1700, 400)}, \n",
    "        #     'b_HeI': {'amp_c':(0.1, 0.08), 'sigma_c':(1400, 1800), 'dv_c':(0, 75)}\n",
    "        # }\n",
    "        \n",
    "        arg_dict = {key: {param: arg_dict_func[key][param](*arg_dict_range[key][param]) for param in arg_dict_func[key]} for key in arg_dict_func}\n",
    "        \n",
    "        wave = np.linspace(4150, 7000, input_width)\n",
    "        flux = generate_spec(wave, arg_dict=arg_dict)\n",
    "        \n",
    "        data = np.stack((wave, flux), axis=0)\n",
    "        X_list.append(torch.tensor(data, dtype=torch.float32).view(1, 2, input_width))\n",
    "        arg_list = [value for line in arg_dict.values() for value in line.values()]\n",
    "        y_list.append(torch.tensor(arg_list, dtype=torch.float32))\n",
    "    \n",
    "    X = torch.cat(X_list, dim=0).reshape(num_samples, 1, 2, input_width)\n",
    "    y = torch.stack(y_list)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, input_height=2, input_width=1000, output_dim=27):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        # 初始标准化\n",
    "        self.bn0 = nn.BatchNorm2d(1)\n",
    "        \n",
    "        # 第一个卷积块：添加残差连接\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(1, 21), padding=(0, 10))\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv1_res = nn.Conv2d(1, 32, kernel_size=(1, 1))  # 1x1卷积用于维度匹配\n",
    "        \n",
    "        # 第二个卷积块\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(1, 21), padding=(0, 10))\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv2_res = nn.Conv2d(32, 64, kernel_size=(1, 1))\n",
    "        \n",
    "        # 第三个卷积块：使用空洞卷积增加感受野\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(1, 21), padding=(0, 20), dilation=(1, 2))\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv3_res = nn.Conv2d(64, 128, kernel_size=(1, 1))\n",
    "        \n",
    "        # 使用注意力机制\n",
    "        self.se_block = SEBlock(128)\n",
    "        \n",
    "        # 池化层\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc_input_dim = self._calculate_fc_input_dim(input_height, input_width)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(self.fc_input_dim, 256)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def _calculate_fc_input_dim(self, input_height, input_width):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_height, input_width)\n",
    "            x = self.pool(self._conv_block3(self.pool(self._conv_block2(self.pool(self._conv_block1(self.bn0(dummy_input)))))))\n",
    "            return x.numel()\n",
    "    \n",
    "    def _conv_block1(self, x):\n",
    "        residual = self.conv1_res(x)\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        return x + residual\n",
    "    \n",
    "    def _conv_block2(self, x):\n",
    "        residual = self.conv2_res(x)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        return x + residual\n",
    "    \n",
    "    def _conv_block3(self, x):\n",
    "        residual = self.conv3_res(x)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = self.se_block(x)\n",
    "        return x + residual\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        \n",
    "        # 卷积模块\n",
    "        x = self._conv_block1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self._conv_block2(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self._conv_block3(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # 全连接模块\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.fc_bn1(self.fc1(x)))\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.fc_bn2(self.fc2(x)))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 添加注意力机制\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedLoss(nn.Module):\n",
    "    def __init__(self, arg_dict_func, arg_dict_range, alpha=0.1, beta=0.1):\n",
    "        super(ImprovedLoss, self).__init__()\n",
    "        self.arg_dict_func = arg_dict_func\n",
    "        self.arg_dict_range = arg_dict_range\n",
    "        \n",
    "        # 根据参数范围设置权重\n",
    "        self.w = []\n",
    "        self.param_indices = {}  # 跟踪参数位置\n",
    "        \n",
    "        idx = 0\n",
    "        for key1, line in arg_dict_func.items():\n",
    "            self.param_indices[key1] = {}\n",
    "            for key2, value in line.items():\n",
    "                self.param_indices[key1][key2] = idx\n",
    "                \n",
    "                if value == uniform:\n",
    "                    self.w.append(arg_dict_range[key1][key2][1] - arg_dict_range[key1][key2][0])\n",
    "                elif value == pnormal or value == normal:\n",
    "                    self.w.append(arg_dict_range[key1][key2][1])\n",
    "                idx += 1\n",
    "                \n",
    "        self.w = torch.tensor(self.w, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # 针对不同参数设置权重\n",
    "        self.importance_weights = torch.ones_like(self.w)\n",
    "        \n",
    "        # 对重要参数增加权重（示例：增加Ha和Hb的权重）\n",
    "        for key1 in ['b_ha', 'b_hb', 'line_o3']:\n",
    "            for key2 in arg_dict_func[key1]:\n",
    "                idx = self.param_indices[key1][key2]\n",
    "                self.importance_weights[idx] = 2.0\n",
    "        \n",
    "        self.alpha = alpha  # L1正则化参数\n",
    "        self.beta = beta    # 平滑项参数\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return x / self.w\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # 归一化目标值\n",
    "        targets_norm = self.normalize(targets)\n",
    "        \n",
    "        # 使用Huber损失结合权重\n",
    "        diff = outputs - targets_norm\n",
    "        mse_loss = torch.pow(diff, 2)\n",
    "        l1_loss = torch.abs(diff)\n",
    "        \n",
    "        # 使用平滑的L1损失\n",
    "        smooth_l1 = torch.where(l1_loss < 1.0, \n",
    "                               0.5 * mse_loss,\n",
    "                               l1_loss - 0.5)\n",
    "        \n",
    "        # 添加权重\n",
    "        weighted_loss = smooth_l1 * self.importance_weights\n",
    "        \n",
    "        # 对不同样本添加不同权重\n",
    "        # 对误差大的样本增加权重\n",
    "        batch_weights = 1.0 + torch.mean(l1_loss, dim=1) * 0.5\n",
    "        \n",
    "        # 添加正则化项，鼓励预测值在合理范围内\n",
    "        l1_reg = torch.mean(torch.abs(outputs))\n",
    "        \n",
    "        # 添加平滑项，惩罚输出的不连续性\n",
    "        smoothness = torch.mean(torch.abs(outputs[:, 1:] - outputs[:, :-1]))\n",
    "        \n",
    "        # 总损失\n",
    "        loss = (torch.mean(weighted_loss * batch_weights.unsqueeze(1)) + \n",
    "                self.alpha * l1_reg + \n",
    "                self.beta * smoothness)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader):\n",
    "    loss = ImprovedLoss(arg_dict_func, arg_dict_range)\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            # 计算相对误差\n",
    "            relative_error = torch.abs(outputs * loss.w - targets) / torch.abs(targets)\n",
    "            correct = torch.all(relative_error < 0.5, dim=1).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += inputs.size(0)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "def plot_training_curve(train_losses, train_accuracies, test_accuracies):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进训练过程\n",
    "def train_improved_model(model, train_loader, test_loader, criterion, num_epochs=400):\n",
    "    # 使用AdamW优化器代替SGD\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # 学习率调度器：余弦退火\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Warmup调度器\n",
    "    warmup_epochs = 10\n",
    "    warmup_scheduler = optim.lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # 梯度累积步数\n",
    "    accumulation_steps = 4\n",
    "    \n",
    "    # EMA模型（指数移动平均）\n",
    "    ema_model = copy.deepcopy(model)\n",
    "    ema_decay = 0.999\n",
    "    \n",
    "    # 早停机制\n",
    "    patience = 30\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 使用进度条\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\") as pbar:\n",
    "            for i, (inputs, targets) in enumerate(pbar):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # 前向传播\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # 缩放损失以适应梯度累积\n",
    "                loss = loss / accumulation_steps\n",
    "                \n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                \n",
    "                # 梯度累积\n",
    "                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                    # 梯度裁剪\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    # 更新参数\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # 更新EMA模型\n",
    "                    with torch.no_grad():\n",
    "                        for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "                            ema_param.data = (ema_decay * ema_param.data + \n",
    "                                            (1 - ema_decay) * param.data)\n",
    "                \n",
    "                epoch_loss += loss.item() * accumulation_steps\n",
    "                pbar.set_postfix({'loss': loss.item() * accumulation_steps})\n",
    "            \n",
    "        # 更新学习率\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # 计算训练集和测试集上的准确率\n",
    "        model.eval()\n",
    "        train_accuracy = calculate_accuracy(model, train_loader)\n",
    "        \n",
    "        # 使用EMA模型进行评估\n",
    "        ema_model.eval()\n",
    "        test_accuracy = calculate_accuracy(ema_model, test_loader)\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, \"\n",
    "                 f\"Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}, \"\n",
    "                 f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            # 保存最佳模型\n",
    "            torch.save(ema_model.state_dict(), \"./model/best_model.pth\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功加载。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 2863/2863 [00:38<00:00, 74.68it/s, loss=3.35]\n",
      "Epoch 2/200: 100%|██████████| 2863/2863 [00:37<00:00, 75.77it/s, loss=1.34]\n",
      "Epoch 3/200: 100%|██████████| 2863/2863 [00:39<00:00, 72.19it/s, loss=1.32]\n",
      "Epoch 4/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.19it/s, loss=1.4] \n",
      "Epoch 5/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.88it/s, loss=1.31]\n",
      "Epoch 6/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.19it/s, loss=1.31]\n",
      "Epoch 7/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.65it/s, loss=1.34]\n",
      "Epoch 8/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.64it/s, loss=1.25]\n",
      "Epoch 9/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.35it/s, loss=1.31]\n",
      "Epoch 10/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.87it/s, loss=1.3] \n",
      "Epoch 11/200:   0%|          | 7/2863 [00:00<00:42, 67.02it/s, loss=1.34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.3089, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.47it/s, loss=1.29]\n",
      "Epoch 12/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.26it/s, loss=1.31]\n",
      "Epoch 13/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.37it/s, loss=1.31]\n",
      "Epoch 14/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.47it/s, loss=1.29]\n",
      "Epoch 15/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.00it/s, loss=1.31]\n",
      "Epoch 16/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.15it/s, loss=1.26]\n",
      "Epoch 17/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.82it/s, loss=1.37]\n",
      "Epoch 18/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.42it/s, loss=1.31]\n",
      "Epoch 19/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.42it/s, loss=1.24]\n",
      "Epoch 20/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.35it/s, loss=1.3] \n",
      "Epoch 21/200:   0%|          | 0/2863 [00:00<?, ?it/s, loss=1.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Loss: 1.3020, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.61it/s, loss=1.24]\n",
      "Epoch 22/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.40it/s, loss=1.36]\n",
      "Epoch 23/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.21it/s, loss=1.27]\n",
      "Epoch 24/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.12it/s, loss=1.3] \n",
      "Epoch 25/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.70it/s, loss=1.35]\n",
      "Epoch 26/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.85it/s, loss=1.28]\n",
      "Epoch 27/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.69it/s, loss=1.31]\n",
      "Epoch 28/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.61it/s, loss=1.33]\n",
      "Epoch 29/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.06it/s, loss=1.27]\n",
      "Epoch 30/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.53it/s, loss=1.28]\n",
      "Epoch 31/200:   0%|          | 8/2863 [00:00<00:39, 71.96it/s, loss=1.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/200], Loss: 1.3014, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.20it/s, loss=1.31]\n",
      "Epoch 32/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.75it/s, loss=1.28]\n",
      "Epoch 33/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.86it/s, loss=1.26]\n",
      "Epoch 34/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.67it/s, loss=1.29]\n",
      "Epoch 35/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.87it/s, loss=1.34]\n",
      "Epoch 36/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.88it/s, loss=1.34]\n",
      "Epoch 37/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.85it/s, loss=1.34]\n",
      "Epoch 38/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.39it/s, loss=1.33]\n",
      "Epoch 39/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.27it/s, loss=1.27]\n",
      "Epoch 40/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.65it/s, loss=1.31]\n",
      "Epoch 41/200:   0%|          | 7/2863 [00:00<00:40, 69.69it/s, loss=1.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/200], Loss: 1.3011, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200: 100%|██████████| 2863/2863 [00:45<00:00, 63.14it/s, loss=1.28]\n",
      "Epoch 42/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.28it/s, loss=1.28]\n",
      "Epoch 43/200: 100%|██████████| 2863/2863 [00:46<00:00, 62.06it/s, loss=1.34]\n",
      "Epoch 44/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.39it/s, loss=1.3] \n",
      "Epoch 45/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.36it/s, loss=1.32]\n",
      "Epoch 46/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.89it/s, loss=1.29]\n",
      "Epoch 47/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.36it/s, loss=1.34]\n",
      "Epoch 48/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.66it/s, loss=1.33]\n",
      "Epoch 49/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.26it/s, loss=1.27]\n",
      "Epoch 50/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.45it/s, loss=1.26]\n",
      "Epoch 51/200:   0%|          | 7/2863 [00:00<00:42, 66.49it/s, loss=1.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200], Loss: 1.3008, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.30it/s, loss=1.37]\n",
      "Epoch 52/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.57it/s, loss=1.35]\n",
      "Epoch 53/200: 100%|██████████| 2863/2863 [00:46<00:00, 62.03it/s, loss=1.28]\n",
      "Epoch 54/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.54it/s, loss=1.31]\n",
      "Epoch 55/200: 100%|██████████| 2863/2863 [00:48<00:00, 59.56it/s, loss=1.32]\n",
      "Epoch 56/200: 100%|██████████| 2863/2863 [00:47<00:00, 60.51it/s, loss=1.32]\n",
      "Epoch 57/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.64it/s, loss=1.32]\n",
      "Epoch 58/200: 100%|██████████| 2863/2863 [00:45<00:00, 62.37it/s, loss=1.27]\n",
      "Epoch 59/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.64it/s, loss=1.31]\n",
      "Epoch 60/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.76it/s, loss=1.22]\n",
      "Epoch 61/200:   0%|          | 8/2863 [00:00<00:41, 69.07it/s, loss=1.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/200], Loss: 1.3006, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.30it/s, loss=1.32]\n",
      "Epoch 62/200: 100%|██████████| 2863/2863 [00:46<00:00, 61.56it/s, loss=1.26]\n",
      "Epoch 63/200: 100%|██████████| 2863/2863 [00:37<00:00, 76.08it/s, loss=1.28]\n",
      "Epoch 64/200: 100%|██████████| 2863/2863 [00:37<00:00, 75.67it/s, loss=1.33]\n",
      "Epoch 65/200: 100%|██████████| 2863/2863 [00:38<00:00, 74.72it/s, loss=1.3] \n",
      "Epoch 66/200: 100%|██████████| 2863/2863 [01:09<00:00, 41.19it/s, loss=1.33]\n",
      "Epoch 67/200: 100%|██████████| 2863/2863 [00:43<00:00, 66.45it/s, loss=1.28]\n",
      "Epoch 68/200: 100%|██████████| 2863/2863 [01:01<00:00, 46.48it/s, loss=1.25]\n",
      "Epoch 69/200: 100%|██████████| 2863/2863 [07:01<00:00,  6.79it/s, loss=1.28]  \n",
      "Epoch 70/200: 100%|██████████| 2863/2863 [00:40<00:00, 70.77it/s, loss=1.33]\n",
      "Epoch 71/200:   0%|          | 7/2863 [00:00<00:41, 68.87it/s, loss=1.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/200], Loss: 1.3005, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/200: 100%|██████████| 2863/2863 [00:40<00:00, 70.22it/s, loss=1.31]\n",
      "Epoch 72/200: 100%|██████████| 2863/2863 [00:42<00:00, 67.23it/s, loss=1.28]\n",
      "Epoch 73/200: 100%|██████████| 2863/2863 [00:41<00:00, 68.90it/s, loss=1.29]\n",
      "Epoch 74/200: 100%|██████████| 2863/2863 [00:41<00:00, 68.36it/s, loss=1.3] \n",
      "Epoch 75/200: 100%|██████████| 2863/2863 [00:40<00:00, 71.15it/s, loss=1.29]\n",
      "Epoch 76/200: 100%|██████████| 2863/2863 [03:02<00:00, 15.69it/s, loss=1.36]\n",
      "Epoch 77/200: 100%|██████████| 2863/2863 [03:12<00:00, 14.85it/s, loss=1.26] \n",
      "Epoch 78/200: 100%|██████████| 2863/2863 [00:40<00:00, 70.04it/s, loss=1.33]\n",
      "Epoch 79/200: 100%|██████████| 2863/2863 [00:41<00:00, 68.82it/s, loss=1.36]\n",
      "Epoch 80/200: 100%|██████████| 2863/2863 [00:41<00:00, 68.84it/s, loss=1.27]\n",
      "Epoch 81/200:   0%|          | 7/2863 [00:00<00:41, 68.44it/s, loss=1.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/200], Loss: 1.3004, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/200: 100%|██████████| 2863/2863 [00:42<00:00, 67.01it/s, loss=1.34]\n",
      "Epoch 82/200: 100%|██████████| 2863/2863 [00:41<00:00, 69.19it/s, loss=1.36]\n",
      "Epoch 83/200: 100%|██████████| 2863/2863 [00:41<00:00, 69.54it/s, loss=1.3] \n",
      "Epoch 84/200: 100%|██████████| 2863/2863 [00:42<00:00, 66.81it/s, loss=1.28]\n",
      "Epoch 85/200: 100%|██████████| 2863/2863 [00:41<00:00, 69.26it/s, loss=1.31]\n",
      "Epoch 86/200: 100%|██████████| 2863/2863 [00:43<00:00, 65.73it/s, loss=1.34]\n",
      "Epoch 87/200: 100%|██████████| 2863/2863 [00:41<00:00, 69.54it/s, loss=1.35]\n",
      "Epoch 88/200: 100%|██████████| 2863/2863 [00:43<00:00, 66.46it/s, loss=1.3] \n",
      "Epoch 89/200: 100%|██████████| 2863/2863 [00:41<00:00, 69.01it/s, loss=1.31]\n",
      "Epoch 90/200: 100%|██████████| 2863/2863 [00:41<00:00, 68.67it/s, loss=1.3] \n",
      "Epoch 91/200:   0%|          | 8/2863 [00:00<00:45, 63.05it/s, loss=1.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/200], Loss: 1.3002, Train Acc: 0.0000, Test Acc: 0.0000, LR: 0.000655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/200: 100%|██████████| 2863/2863 [00:43<00:00, 65.67it/s, loss=1.25]\n",
      "Epoch 92/200: 100%|██████████| 2863/2863 [00:42<00:00, 67.55it/s, loss=1.27]\n",
      "Epoch 93/200:  19%|█▉        | 540/2863 [00:07<00:34, 67.58it/s, loss=1.3] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m criterion \u001b[38;5;241m=\u001b[39m ImprovedLoss(arg_dict_func, arg_dict_range)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 训练改进的模型\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m train_losses, train_accuracies, test_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_improved_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 绘制训练过程\u001b[39;00m\n\u001b[0;32m     43\u001b[0m plot_training_curve(train_losses, train_accuracies, test_accuracies)\n",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mtrain_improved_model\u001b[1;34m(model, train_loader, test_loader, criterion, num_epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m     43\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 44\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 缩放损失以适应梯度累积\u001b[39;00m\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m accumulation_steps\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\sagan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\sagan\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36mImprovedLoss.forward\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m     47\u001b[0m l1_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(diff)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 使用平滑的L1损失\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m smooth_l1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml1_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m                       \u001b[49m\u001b[43ml1_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 添加权重\u001b[39;00m\n\u001b[0;32m     55\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m smooth_l1 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_weights\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 使用改进后的方法训练模型\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 超参数\n",
    "    input_height = 2\n",
    "    input_width = 1000\n",
    "    output_dim = 27\n",
    "    num_samples = 114514\n",
    "    batch_size = 32\n",
    "    num_epochs = 200\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    try:\n",
    "        X = torch.load(\"./data_generated/X.pt\")\n",
    "        y = torch.load(\"./data_generated/y.pt\")\n",
    "        print(\"数据已成功加载。\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"未找到数据，正在生成数据...\")\n",
    "        X, y = generate_data(arg_dict_func, arg_dict_range, num_samples, input_width)\n",
    "        # 保存数据\n",
    "        torch.save(X, \"./data_generated/X.pt\")\n",
    "        torch.save(y, \"./data_generated/y.pt\")\n",
    "        print(\"数据已成功生成并保存。\")\n",
    "    \n",
    "    dataset = TensorDataset(X.to(device), y.to(device))\n",
    "    train_size = int(0.8 * num_samples)\n",
    "    test_size = num_samples - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # 初始化改进的模型和损失函数\n",
    "    model = ImprovedCNN(input_height, input_width, output_dim).to(device)\n",
    "    criterion = ImprovedLoss(arg_dict_func, arg_dict_range)\n",
    "    \n",
    "    # 训练改进的模型\n",
    "    train_losses, train_accuracies, test_accuracies = train_improved_model(\n",
    "        model, train_loader, test_loader, criterion, num_epochs=num_epochs\n",
    "    )\n",
    "    \n",
    "    # 绘制训练过程\n",
    "    plot_training_curve(train_losses, train_accuracies, test_accuracies)\n",
    "    \n",
    "    # 保存模型\n",
    "    model_name = 'cnn2'\n",
    "    torch.save(model.state_dict(), f\"./model/{model_name}.pth\")\n",
    "    print(f\"Model saved to ./model/{model_name}.pth\")\n",
    "    \n",
    "    # 加载模型并测试\n",
    "    model.load_state_dict(torch.load(f\"./model/{model_name}.pth\"))\n",
    "    test_accuracy = calculate_accuracy(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
